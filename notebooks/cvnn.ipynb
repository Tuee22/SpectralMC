{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb9e432-a4f2-4b9b-8693-c0aa221d6e8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# modules generated in https://chat.openai.com/share/e87f6356-4272-4e28-9d73-a6175aba09c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d484f-ac2e-4775-8a6c-bd6f6adf608f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d69434-cf36-4de1-b9d8-48f461222bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class zReLU(nn.Module):\n",
    "    def forward(self, input):\n",
    "        # Create a mask where both real and imaginary parts are positive\n",
    "        mask = (input.real >= 0) & (input.imag >= 0)\n",
    "        # Apply the mask\n",
    "        return input * mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1908093-edb0-457c-bc31-7936a5b9e790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class modReLU(nn.Module):\n",
    "    def __init__(self, bias_init=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the modReLU module.\n",
    "\n",
    "        Parameters:\n",
    "            bias_init (float): Initial value for the bias parameter.\n",
    "        \"\"\"\n",
    "        super(modReLU, self).__init__()\n",
    "        # Initialize the bias as a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.tensor([bias_init]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the modReLU activation.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): Complex input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying modReLU activation.\n",
    "        \"\"\"\n",
    "        modulus = input.abs()  # Compute the modulus of the complex input\n",
    "        phase = input.angle()  # Compute the phase of the complex input\n",
    "\n",
    "        # Apply the modReLU function: ReLU(modulus + bias) * e^(j*phase)\n",
    "        # Ensure the bias is broadcasted correctly over the input dimensions\n",
    "        activated_modulus = torch.relu(modulus + self.bias)\n",
    "        output = torch.polar(activated_modulus, phase)  # Reconstruct the complex number\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4fe320-d438-46cc-8e93-45a07c8a0ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias_init=0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features, out_features)\n",
    "        self.bn1 = nn.BatchNorm1d(out_features)  # BatchNorm after the first linear layer\n",
    "        self.modrelu1 = modReLU(bias_init)\n",
    "        self.linear2 = nn.Linear(out_features, out_features)\n",
    "        self.bn2 = nn.BatchNorm1d(out_features)  # BatchNorm after the second linear layer\n",
    "        self.modrelu2 = modReLU(bias_init)\n",
    "        # Adjust dimensions if necessary, with batch norm\n",
    "        if in_features != out_features:\n",
    "            self.match_dimensions = nn.Sequential(\n",
    "                nn.Linear(in_features, out_features),\n",
    "                nn.BatchNorm1d(out_features)\n",
    "            )\n",
    "        else:\n",
    "            self.match_dimensions = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.linear1(x)\n",
    "        out = self.bn1(out)  # Apply batch norm\n",
    "        out = self.modrelu1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.bn2(out)  # Apply batch norm\n",
    "        out = self.modrelu2(out)\n",
    "        if self.match_dimensions is not None:\n",
    "            residual = self.match_dimensions(residual)\n",
    "        out += residual  # Add input to the output\n",
    "        return out\n",
    "\n",
    "class CVNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(CVNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        previous_layer_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(ResidualBlock(previous_layer_size, hidden_size))\n",
    "            previous_layer_size = hidden_size\n",
    "            \n",
    "        # Add batch norm before the final linear layer if necessary\n",
    "        if hidden_sizes:\n",
    "            self.layers.append(nn.BatchNorm1d(previous_layer_size))\n",
    "        self.final_linear = nn.Linear(previous_layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.final_linear(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_size = 10  # Example input size\n",
    "hidden_sizes = [20, 30]  # Example sizes of hidden layers\n",
    "output_size = 5  # Example output size\n",
    "\n",
    "model = CVNN(input_size, hidden_sizes, output_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b1388-59ad-400f-ad64-8e5ea8b09e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ComplexBatchNorm(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ComplexBatchNorm, self).__init__()\n",
    "        self.mag_bn = nn.BatchNorm1d(num_features)\n",
    "        # Using a placeholder for phase normalization; consider circular statistics for actual implementation\n",
    "        self.phase_bn = nn.BatchNorm1d(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        magnitude = torch.abs(x)\n",
    "        phase = torch.angle(x)\n",
    "\n",
    "        normalized_mag = self.mag_bn(magnitude)\n",
    "        normalized_phase = self.phase_bn(phase)  # Placeholder\n",
    "\n",
    "        # Reconstruct complex numbers from normalized magnitude and phase\n",
    "        x_normalized = torch.polar(normalized_mag, normalized_phase)\n",
    "        return x_normalized\n",
    "\n",
    "class modReLU(nn.Module):\n",
    "    def __init__(self, bias_init=0.1):\n",
    "        super(modReLU, self).__init__()\n",
    "        self.bias = nn.Parameter(torch.tensor([bias_init]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Adapted for complex inputs\n",
    "        magnitude = torch.abs(input)\n",
    "        activated_modulus = F.relu(magnitude + self.bias)\n",
    "        phase = torch.angle(input)\n",
    "        return torch.polar(activated_modulus, phase)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias_init=0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features, out_features)\n",
    "        self.bn1 = ComplexBatchNorm(out_features)  # Complex BatchNorm\n",
    "        self.modrelu1 = modReLU(bias_init)\n",
    "        self.linear2 = nn.Linear(out_features, out_features)\n",
    "        self.bn2 = ComplexBatchNorm(out_features)  # Complex BatchNorm\n",
    "        self.modrelu2 = modReLU(bias_init)\n",
    "\n",
    "        if in_features != out_features:\n",
    "            self.match_dimensions = nn.Sequential(\n",
    "                nn.Linear(in_features, out_features),\n",
    "                ComplexBatchNorm(out_features)  # Complex BatchNorm for matching dimensions\n",
    "            )\n",
    "        else:\n",
    "            self.match_dimensions = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.linear1(x)\n",
    "        out = self.bn1(out)  # Apply complex batch norm\n",
    "        out = self.modrelu1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.bn2(out)  # Apply complex batch norm\n",
    "        out = self.modrelu2(out)\n",
    "        if self.match_dimensions is not None:\n",
    "            residual = self.match_dimensions(residual)\n",
    "        out += residual  # Add input to the output\n",
    "        return out\n",
    "\n",
    "class CVNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(CVNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        previous_layer_size = input_size\n",
    "\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(ResidualBlock(previous_layer_size, hidden_size))\n",
    "            previous_layer_size = hidden_size\n",
    "\n",
    "        self.final_linear = nn.Linear(previous_layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.final_linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878642b-7c4e-414e-8dec-a4e5aa74ae5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CircularStatistics(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super(CircularStatistics, self).__init__()\n",
    "        self.eps = eps\n",
    "        # Learnable parameters for circular data\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, phase):\n",
    "        # Convert phase to complex representation\n",
    "        complex_phase = torch.exp(1j * phase)\n",
    "\n",
    "        # Circular mean\n",
    "        circular_mean = torch.atan2(complex_phase.imag.mean(dim=0), complex_phase.real.mean(dim=0))\n",
    "\n",
    "        # Circular variance (using length of mean resultant vector)\n",
    "        norm = torch.abs(complex_phase.mean(dim=0))\n",
    "        circular_variance = 1 - norm\n",
    "\n",
    "        # Normalize phase\n",
    "        phase_normalized = phase - circular_mean.unsqueeze(0)\n",
    "        phase_normalized = torch.atan2(torch.sin(phase_normalized), torch.cos(phase_normalized))  # Wrap-around effect\n",
    "\n",
    "        # Apply learnable parameters\n",
    "        phase_normalized = self.gamma * (phase_normalized / (circular_variance.sqrt().unsqueeze(0) + self.eps)) + self.beta\n",
    "\n",
    "        return phase_normalized\n",
    "\n",
    "class ComplexBatchNorm(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ComplexBatchNorm, self).__init__()\n",
    "        self.mag_bn = nn.BatchNorm1d(num_features)  # BatchNorm for magnitude\n",
    "        self.phase_bn = CircularStatistics(num_features)  # Circular statistics for phase\n",
    "\n",
    "    def forward(self, x):\n",
    "        magnitude = torch.abs(x)\n",
    "        phase = torch.angle(x)\n",
    "\n",
    "        # Normalize magnitude with standard BatchNorm\n",
    "        normalized_mag = self.mag_bn(magnitude)\n",
    "        \n",
    "        # Normalize phase with CircularStatistics\n",
    "        normalized_phase = self.phase_bn(phase)\n",
    "\n",
    "        # Reconstruct complex numbers\n",
    "        x_normalized = torch.polar(normalized_mag, normalized_phase)\n",
    "        return x_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3cce1-cfba-485b-aa00-5263d979873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggested baseline\n",
    "class SimpleCVNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(SimpleCVNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Starting layer sizes configuration\n",
    "        previous_layer_size = input_size\n",
    "        \n",
    "        # Constructing the hidden layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(previous_layer_size, hidden_size))\n",
    "            self.layers.append(nn.ReLU())  # Using ReLU for simplicity; replace with a complex-compatible activation if necessary\n",
    "            previous_layer_size = hidden_size\n",
    "        \n",
    "        # Final layer to produce the output\n",
    "        self.final_linear = nn.Linear(previous_layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # Apply each layer in sequence\n",
    "        x = self.final_linear(x)\n",
    "        return x\n",
    "\n",
    "# Example model instantiation\n",
    "input_size = 100  # Number of input features\n",
    "hidden_sizes = [128, 64]  # Sizes of hidden layers\n",
    "output_size = 10  # Number of output features\n",
    "\n",
    "model = SimpleCVNN(input_size, hidden_sizes, output_size)\n",
    "\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
