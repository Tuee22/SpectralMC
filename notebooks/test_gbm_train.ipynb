{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fa1b11",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GPU Black-Scholes Trainer with SpectralMC\n",
    "This interactive Jupyter notebook demonstrates how to:\n",
    "1. Define a **Black-Scholes** simulation via `spectralmc.gbm`.\n",
    "2. Construct a **CVNN** from `spectralmc.cvnn`.\n",
    "3. Train via the **GbmTrainer** in `spectralmc.gbm_trainer`.\n",
    "4. Track and visualize the training progress with **TensorBoard**.\n",
    "5. Predict final prices on sample inputs.\n",
    "\n",
    "Make sure you have:\n",
    "- A CUDA-capable GPU.\n",
    "- The `spectralmc` package installed (with your `sobol_sampler.py`, `gbm.py`, `cvnn.py`, `gbm_trainer.py`).\n",
    "- The `tensorboard` Python package installed.\n",
    "\n",
    "We will do a brief training run, then use TensorBoard to visualize the loss.\n",
    "\n",
    "> **Note**: The large number of MC paths may require significant GPU memory if you pick large parameters. Consider smaller \"batches_per_mc_run\" or \"network_size\" if your GPU is memory-limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04597da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import cupy as cp  # type: ignore[import-untyped]\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from spectralmc.gbm import SimulationParams, BlackScholes\n",
    "from spectralmc.cvnn import CVNN\n",
    "from spectralmc.gbm_trainer import GbmTrainer, _inputs_to_real_imag\n",
    "from spectralmc.sobol_sampler import BoundSpec\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available?:\", torch.cuda.is_available())\n",
    "\n",
    "# If needed, ensure your environment has:\n",
    "# pip install tensorboard\n",
    "# to allow in-notebook visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01b1ec",
   "metadata": {},
   "source": [
    "## Set up TensorBoard\n",
    "We'll create a new log directory each run to keep logs separate.\n",
    "You can adjust the path as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d854e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"./.logs/tb_run_\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "print(f\"TensorBoard log directory: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890493a1",
   "metadata": {},
   "source": [
    "## Define Training Configuration\n",
    "We'll pick a small-ish Monte Carlo config to ensure it runs quickly. If you have a lot of GPU memory, you can increase `batches_per_mc_run` or `network_size`.\n",
    "\n",
    "We also define the domain for `(X0,K,T,r,d,v)` and create our **CVNN** with certain hidden features. Finally, we instantiate the **GbmTrainer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_params = SimulationParams(\n",
    "    timesteps=16,\n",
    "    network_size=8,\n",
    "    batches_per_mc_run=256,  # total_paths = 8*256= 2048\n",
    "    threads_per_block=128,\n",
    "    mc_seed=123,\n",
    "    buffer_size=1,\n",
    "    dtype=\"float32\",  # can switch to \"float64\" if desired\n",
    "    simulate_log_return=True,\n",
    "    normalize_forwards=False,\n",
    ")\n",
    "\n",
    "domain_example = {\n",
    "    \"X0\": BoundSpec(lower=50.0, upper=150.0),\n",
    "    \"K\":  BoundSpec(lower=50.0, upper=150.0),\n",
    "    \"T\":  BoundSpec(lower=0.1,  upper=2.0),\n",
    "    \"r\":  BoundSpec(lower=0.0,  upper=0.1),\n",
    "    \"d\":  BoundSpec(lower=0.0,  upper=0.05),\n",
    "    \"v\":  BoundSpec(lower=0.1,  upper=0.5),\n",
    "}\n",
    "\n",
    "cvnn_net = CVNN(\n",
    "    input_features=6,\n",
    "    output_features=sim_params.network_size,\n",
    "    hidden_features=16,\n",
    "    num_residual_blocks=1,\n",
    ")\n",
    "\n",
    "trainer = GbmTrainer(\n",
    "    sim_params=sim_params,\n",
    "    domain_bounds=domain_example,\n",
    "    skip_sobol=0,\n",
    "    sobol_seed=42,\n",
    "    cvnn=cvnn_net,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e880636",
   "metadata": {},
   "source": [
    "## Train and Log the Loss to TensorBoard\n",
    "We'll do a short run of, say, 30 batches. Each batch has 8-16 Sobol points. Then we write the loss each step into TensorBoard logs. The final loss is printed each 10 steps.\n",
    "\n",
    "To view TensorBoard inside Jupyter, we can run `%load_ext tensorboard` then `%tensorboard --logdir=...`\n",
    "If that doesn't work, run `tensorboard --logdir=logs/...` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125857c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# We'll define a train_and_log function that uses the trainer.\n\ndef train_and_log(\n    trainer: GbmTrainer,\n    num_batches: int = 30,\n    batch_size: int = 8,\n    learning_rate: float = 1e-3,\n    writer=writer,\n) -> None:\n    \"\"\"Train the CVNN for a few batches, logging loss to TensorBoard.\"\"\"\n    import torch.nn.functional as F\n\n    sim_params = trainer.sim_params\n    # Decide complex dtype from sim_params\n    cupy_complex_dtype = cp.complex64 if sim_params.dtype == \"float32\" else cp.complex128\n    torch_complex_dtype = torch.complex64 if sim_params.dtype == \"float32\" else torch.complex128\n    torch_real_dtype = torch.float32 if sim_params.dtype == \"float32\" else torch.float64\n\n    trainer.cvnn.train()\n    optimizer = torch.optim.Adam(trainer.cvnn.parameters(), lr=learning_rate)\n\n    for step in range(1, num_batches + 1):\n        sobol_points = trainer.sampler.sample(batch_size)\n\n        payoff_fft_cp = cp.zeros(\n            (batch_size, sim_params.network_size), dtype=cupy_complex_dtype\n        )\n\n        for i, bs_input in enumerate(sobol_points):\n            pr = trainer.bsm_engine.price(inputs=bs_input)\n            put_price_cp = pr.put_price\n            put_mat = put_price_cp.reshape(\n                (sim_params.batches_per_mc_run, sim_params.network_size)\n            )\n            put_fft = cp.fft.fft(put_mat, axis=1)\n            payoff_mean_fft = cp.mean(put_fft, axis=0)\n            payoff_fft_cp[i, :] = payoff_mean_fft\n\n        # Convert CuPy->Torch\n        payoff_fft_torch = torch.from_dlpack(payoff_fft_cp)\n        payoff_fft_torch = payoff_fft_torch.to(torch_complex_dtype)\n\n        target_real = payoff_fft_torch.real\n        target_imag = payoff_fft_torch.imag\n\n        real_in, imag_in = _inputs_to_real_imag(\n            sobol_points, dtype=torch_real_dtype, device=trainer.device\n        )\n\n        pred_r, pred_i = trainer.cvnn(real_in, imag_in)\n        loss_r = F.mse_loss(pred_r, target_real)\n        loss_i = F.mse_loss(pred_i, target_imag)\n        loss_val = loss_r + loss_i\n\n        optimizer.zero_grad()\n        loss_val.backward()  # type: ignore[no-untyped-call]\n        optimizer.step()\n\n        # Write to TensorBoard\n        writer.add_scalar(\"Loss/train\", loss_val.item(), step)\n\n        if step % 10 == 0 or step == num_batches:\n            print(f\"[TRAIN] step={step}/{num_batches}, loss={loss_val.item():.6f}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91176eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's do a short training run, e.g. 30 steps with batch_size=8\n",
    "train_and_log(trainer, num_batches=30, batch_size=8, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3095267",
   "metadata": {},
   "source": [
    "### Launch TensorBoard (Optional)\n",
    "If you're in Jupyter, you can try:\n",
    "```\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs\n",
    "```\n",
    "If that doesn't work, you can run:\n",
    "```\n",
    "tensorboard --logdir=./logs\n",
    "```\n",
    "in a separate terminal, then open the displayed link in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860985e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment these lines if you want to see TensorBoard inline (sometimes it doesn't work well in certain Jupyter envs):\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50cb473",
   "metadata": {},
   "source": [
    "## Predict on sample inputs\n",
    "We'll now see how to do inference for 2 custom `(X0,K,T,r,d,v)` sets. The `predict_price` method\n",
    "will ifft the network’s DFT output and interpret the 0-frequency component as the put price. If there's\n",
    "a non-trivial imaginary component, it’ll raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44dbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_inputs = [\n",
    "    BlackScholes.Inputs(X0=100.0, K=100.0, T=1.0, r=0.02, d=0.01, v=0.2),\n",
    "    BlackScholes.Inputs(X0=110.0, K=105.0, T=0.5, r=0.03, d=0.0, v=0.3)\n",
    "]\n",
    "results = trainer.predict_price(sample_inputs)\n",
    "for idx, res in enumerate(results):\n",
    "    print(f\"Sample {idx}: put={res.put_price:.4f}, call={res.call_price:.4f}, \"\n",
    "          f\"underlying={res.underlying:.4f}, put_convexity={res.put_convexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354018e",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- We have set up a short training loop for GPU-based MC via `BlackScholes`.\n",
    "- We used `CVNN` to learn the payoff DFT, logging the training progress to TensorBoard.\n",
    "- We can visualize the training progress by opening TensorBoard and checking the `Loss/train` plot.\n",
    "\n",
    "If you see large imaginary parts in `predict_price`, it might indicate the model hasn't learned a purely real distribution or needs more training.\n",
    "Feel free to tweak the hyperparameters (like `learning_rate`, `timesteps`, `network_size`) to get better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}