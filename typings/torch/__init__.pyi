"""
Strict, minimal stub for the public ``torch`` namespace as exercised by
*SpectralMC*.  Every declared symbol is *fully typed* so that
``mypy --strict`` succeeds without ``Any``, ``cast`` or ``ignore``.
"""

from __future__ import annotations

import builtins as _b
from typing import Iterator, Sequence, Tuple, TypeAlias, TypeVar, overload

# real import kept only for run-time singletons
import torch as _torch_runtime  # noqa: F401

# ─────────────────────────── dtype & device ────────────────────────────
class dtype: ...

float32: dtype
float64: dtype
float16: dtype
bfloat16: dtype
complex64: dtype
complex128: dtype
int64: dtype
float: dtype
double: dtype
long: dtype

def get_default_dtype() -> dtype: ...
def set_default_dtype(d: dtype) -> None: ...

class device:
    """Stub of run-time ``torch.device`` (context-manager & comparable)."""

    def __init__(self, spec: str | device | None = ...) -> None: ...
    def __enter__(self) -> device: ...
    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc_val: BaseException | None,
        exc_tb: object | None,
    ) -> bool | None: ...
    @property
    def type(self) -> str: ...
    @property
    def index(self) -> int | None: ...

_TorchDevice: TypeAlias = device
_DType = dtype  # shorthand

# ───────────────────────────────── Tensor ──────────────────────────────
_TTensor = TypeVar("_TTensor", bound="Tensor")

class Tensor:
    """Subset of ``torch.Tensor`` sufficient for this project."""

    # construction / autograd ------------------------------------------------
    def __init__(
        self,
        *size: int,
        dtype: _DType | None = ...,
        requires_grad: bool = ...,
    ) -> None: ...
    def backward(self, gradient: Tensor | None = ...) -> None: ...

    grad: Tensor | None
    requires_grad: bool

    # shape & stats ----------------------------------------------------------
    @property
    def dtype(self) -> _DType: ...
    @property
    def shape(self) -> tuple[int, ...]: ...
    @property
    def ndim(self) -> int: ...
    def mean(self, dim: int | None = ..., **kw: object) -> Tensor: ...
    def var(
        self, dim: int | None = ..., *, unbiased: bool = ..., **kw: object
    ) -> Tensor: ...
    def max(self, dim: int | None = ..., keepdim: bool = ...) -> Tensor: ...
    def sum(
        self,
        dim: int | None = ...,
        keepdim: bool = ...,
        *,
        dtype: _DType | None = ...,
    ) -> Tensor: ...
    def unsqueeze(self, dim: int) -> Tensor: ...
    def squeeze(self, dim: int | None = ...) -> Tensor: ...
    def transpose(self, dim0: int, dim1: int) -> Tensor: ...
    @property
    def T(self) -> Tensor: ...

    # element-wise helpers ---------------------------------------------------
    def abs(self) -> Tensor: ...
    def pow(self, exponent: _b.int | _b.float) -> Tensor: ...
    def square(self) -> Tensor: ...
    def all(self) -> Tensor: ...

    # arithmetic -------------------------------------------------------------
    def __add__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    __radd__ = __add__
    def __sub__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    __rsub__ = __sub__
    def __mul__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    __rmul__ = __mul__
    def __truediv__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __rtruediv__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __matmul__(self, other: Tensor) -> Tensor: ...

    # comparisons / masks ----------------------------------------------------
    def __lt__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __le__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __gt__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __ge__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __and__(self, other: Tensor | bool) -> Tensor: ...
    __rand__ = __and__

    def __getitem__(self, item: object) -> Tensor: ...

    # in-place ops -----------------------------------------------------------
    def mul_(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def add_(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def copy_(self, other: Tensor) -> Tensor: ...
    def zero_(self) -> Tensor: ...
    def fill_(self, value: _b.int | _b.float) -> Tensor: ...

    # misc helpers -----------------------------------------------------------
    def detach(self) -> Tensor: ...
    def cpu(self) -> Tensor: ...
    def clone(self) -> Tensor: ...
    def reshape(
        self, shape: tuple[int, ...] | Sequence[int] | int, *more: int
    ) -> Tensor: ...
    def tolist(self) -> list[_b.int | _b.float]: ...
    def item(self) -> _b.int | _b.float: ...
    def __float__(self) -> _b.float: ...

    # extra attributes -------------------------------------------------------
    @property
    def real(self) -> Tensor: ...
    @property
    def imag(self) -> Tensor: ...
    @property
    def device(self) -> _TorchDevice: ...
    def to(
        self: _TTensor,
        dtype: _DType | None = ...,
        device: _TorchDevice | str | None = ...,
        copy: bool | None = ...,
        non_blocking: bool | None = ...,
    ) -> _TTensor: ...
    def equal(self, other: Tensor) -> bool: ...
    def __iter__(self) -> Iterator[Tensor]: ...

# ────────────────────────── functional helpers ──────────────────────────
def zeros(*size: int, dtype: _DType | None = ...) -> Tensor: ...
def ones(
    *size: int, dtype: _DType | None = ..., device: _TorchDevice | str | None = ...
) -> Tensor: ...
def full(
    size: Tuple[int, ...], fill_value: _b.int | _b.float, dtype: _DType | None = ...
) -> Tensor: ...
def full_like(
    a: Tensor,
    fill_value: _b.int | _b.float,
    *,
    dtype: _DType | None = ...,
    device: _TorchDevice | str | None = ...,
) -> Tensor: ...
def zeros_like(a: Tensor) -> Tensor: ...
def matmul(a: Tensor, b: Tensor) -> Tensor: ...
def sqrt(a: Tensor) -> Tensor: ...
def clamp(
    a: Tensor, *, min: _b.float | None = ..., max: _b.float | None = ...
) -> Tensor: ...
def relu(a: Tensor) -> Tensor: ...
def stack(tensors: Sequence[Tensor], dim: int = ...) -> Tensor: ...
def square(a: Tensor) -> Tensor: ...
def all(a: Tensor) -> bool: ...
def allclose(
    a: Tensor, b: Tensor, *, atol: _b.float = ..., rtol: _b.float = ...
) -> bool: ...
def isfinite(a: Tensor) -> Tensor: ...
def complex(real: Tensor, imag: Tensor, *, dtype: _DType | None = ...) -> Tensor: ...
def real(input: Tensor) -> Tensor: ...
def imag(input: Tensor) -> Tensor: ...
@overload
def max(input: Tensor) -> Tensor: ...
@overload
def max(input: Tensor, other: Tensor) -> Tensor: ...
@overload
def max(input: Tensor, dim: int, keepdim: bool = ...) -> Tuple[Tensor, Tensor]: ...

abs = Tensor.abs

def equal(a: Tensor, b: Tensor) -> bool: ...

# randomness --------------------------------------------------------------
def manual_seed(seed: int) -> None: ...
def randn(
    *size: int,
    dtype: _DType | None = ...,
    device: _TorchDevice | str | None = ...,
    requires_grad: bool = ...,
) -> Tensor: ...
@overload
def tensor(
    data: _b.int | _b.float,
    *,
    dtype: _DType | None = ...,
    device: _TorchDevice | str | None = ...,
    requires_grad: bool = ...,
) -> Tensor: ...
@overload
def tensor(
    data: Sequence[_b.int | _b.float],
    *,
    dtype: _DType | None = ...,
    device: _TorchDevice | str | None = ...,
    requires_grad: bool = ...,
) -> Tensor: ...
@overload
def tensor(
    data: Sequence[Sequence[_b.int | _b.float]],
    *,
    dtype: _DType | None = ...,
    device: _TorchDevice | str | None = ...,
    requires_grad: bool = ...,
) -> Tensor: ...

# autograd guard ----------------------------------------------------------
class _NoGrad:
    def __enter__(self) -> None: ...
    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc_val: BaseException | None,
        exc_tb: object | None,
    ) -> None: ...

def no_grad() -> _NoGrad: ...

# tiny sub-modules --------------------------------------------------------
class _FFTModule:
    def fft(self, input: Tensor, n: int | None = ..., dim: int = ...) -> Tensor: ...
    def ifft(self, input: Tensor, n: int | None = ..., dim: int = ...) -> Tensor: ...

fft: _FFTModule

class _LinalgModule:
    def eigh(self, input: Tensor) -> Tuple[Tensor, Tensor]: ...

linalg: _LinalgModule

# re-export stub sub-packages so ``import torch.nn`` works ----------------
from . import nn as nn  # noqa: F401
from . import optim as optim  # noqa: F401
from . import cuda as cuda  # noqa: F401
from . import utils as utils  # re-export
