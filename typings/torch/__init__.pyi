from __future__ import annotations

from types import ModuleType
from typing import Sequence, Tuple
import builtins as _b  # keep built-in float / int distinct

# ────── value-level dtype sentinels ──────
class dtype: ...

float32: dtype
float64: dtype
int64: dtype

# runtime aliases that real torch exports
float: dtype  # torch.float32
double: dtype  # torch.float64
long: dtype  # torch.int64

# ────── minimal Tensor object (only methods/operators used) ──────
class Tensor:
    # construction
    def __init__(
        self,
        *size: int,
        dtype: dtype | None = ...,
        requires_grad: bool = ...,
    ) -> None: ...

    # shape helpers
    def mean(self, dim: int | None = ..., **kw: object) -> Tensor: ...
    def unsqueeze(self, dim: int) -> Tensor: ...
    def squeeze(self, dim: int | None = ...) -> Tensor: ...
    def transpose(self, dim0: int, dim1: int) -> Tensor: ...
    @property
    def T(self) -> Tensor: ...

    # in-place ops
    def mul_(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def add_(self, other: Tensor | _b.float | _b.int) -> Tensor: ...

    # arithmetic
    def __add__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    __radd__ = __add__
    def __sub__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    __rsub__ = __sub__
    def __mul__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    __rmul__ = __mul__
    def __truediv__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __rtruediv__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __matmul__(self, other: Tensor) -> Tensor: ...

    # comparisons → boolean masks
    def __lt__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __le__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __gt__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __ge__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...

    # logical masks
    def __and__(self, other: Tensor | bool) -> Tensor: ...
    __rand__ = __and__

    # indexing
    def __getitem__(self, item: object) -> Tensor: ...

# ────── functional helpers referenced in cvnn.py ──────
def zeros(*size: int, dtype: dtype | None = ...) -> Tensor: ...
def full(
    size: Tuple[int, ...],
    fill_value: _b.int | _b.float,
    dtype: dtype | None = ...,
) -> Tensor: ...
def matmul(a: Tensor, b: Tensor) -> Tensor: ...
def sqrt(a: Tensor) -> Tensor: ...
def clamp(
    a: Tensor,
    *,
    min: _b.int | _b.float | None = ...,
    max: _b.int | _b.float | None = ...,
) -> Tensor: ...
def relu(a: Tensor) -> Tensor: ...
def stack(tensors: Sequence[Tensor], dim: int = ...) -> Tensor: ...

# ────── no_grad context manager ──────
class _NoGrad:
    def __enter__(self) -> None: ...
    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc: BaseException | None,
        tb: object | None,
    ) -> None: ...

def no_grad() -> _NoGrad: ...

# ────── placeholder sub-modules so that
#        import torch.nn as nn
#        torch.linalg.eigh
#        both resolve (their detailed stubs live below) ──────
nn: ModuleType
linalg: ModuleType
