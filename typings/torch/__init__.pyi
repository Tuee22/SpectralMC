# typings/torch/__init__.pyi
"""
Strict, project‑specific stub for the **top‑level** :pymod:`torch` namespace.

Only the public surface exercised by SpectralMC is declared.  The stub must
remain *type‑pure*: **no** ``Any``, **no** ``cast``, **no** ``type: ignore``.
"""

from __future__ import annotations

import builtins as _b
from typing import (
    Iterator,
    Mapping,
    Sequence,
    Tuple,
    TypeAlias,
    TypeVar,
    overload,
    Protocol,
)

# ─────────────────────────── dtype & device singletons ────────────────────
class dtype: ...

float32: dtype
float64: dtype
float16: dtype
bfloat16: dtype
complex64: dtype
complex128: dtype
int64: dtype
float: dtype  # legacy aliases
double: dtype
long: dtype

def get_default_dtype() -> dtype: ...
def set_default_dtype(d: dtype) -> None: ...
def set_default_device(d: str | "device") -> None: ...

class device:
    """
    Minimal façade of :class:`torch.device`.  Two‑argument construction
    (``torch.device("cuda", 0)``) is required by the unit‑tests.
    """

    def __init__(
        self,
        spec_or_type: str | "device" | None = ...,
        index: int | None = ...,
    ) -> None: ...
    def __enter__(self) -> "device": ...
    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc_val: BaseException | None,
        exc_tb: object | None,
    ) -> bool | None: ...
    @property
    def type(self) -> str: ...
    @property
    def index(self) -> int | None: ...

_TorchDevice: TypeAlias = device
_DType = dtype

# ───────────────────────────────── Tensor ─────────────────────────────────
# -------------------------------------------------------------------- #
#  Helper describing PyTorch’s untyped storage                         #
# -------------------------------------------------------------------- #
class _UntypedStorage(Protocol):
    def data_ptr(self) -> int: ...

_TTensor = TypeVar("_TTensor", bound="Tensor")

class Tensor:
    """Reduced :class:`torch.Tensor` covering everything SpectralMC touches."""

    # Construction & autograd ------------------------------------------------
    def __init__(
        self,
        *size: int,
        dtype: _DType | None = ...,
        requires_grad: bool = ...,
    ) -> None: ...

    grad: "Tensor | None"
    requires_grad: bool

    def backward(self, gradient: "Tensor | None" = ...) -> None: ...

    # Core properties --------------------------------------------------------
    @property
    def dtype(self) -> _DType: ...
    @property
    def shape(self) -> tuple[int, ...]: ...
    @property
    def ndim(self) -> int: ...
    @property
    def device(self) -> _TorchDevice: ...
    @property
    def T(self) -> "Tensor": ...
    def type(self, t: str | None = ...) -> str | "Tensor": ...

    # Presence checks used in tests
    def is_floating_point(self) -> bool: ...
    @property
    def is_cuda(self) -> bool: ...

    # Math helpers -----------------------------------------------------------
    def mean(self, dim: int | None = ..., **kw: object) -> "Tensor": ...
    def var(
        self,
        dim: int | None = ...,
        *,
        unbiased: bool = ...,
        **kw: object,
    ) -> "Tensor": ...
    def pow(self, exponent: _b.int | _b.float | "Tensor") -> "Tensor": ...
    def square(self) -> "Tensor": ...
    def abs(self) -> "Tensor": ...
    def all(self) -> "Tensor": ...

    # Vectorised ** operator
    def __pow__(self, exponent: _b.int | _b.float | "Tensor") -> "Tensor": ...
    def __rpow__(self, exponent: _b.int | _b.float | "Tensor") -> "Tensor": ...

    # Reductions / transforms ------------------------------------------------
    def max(self, dim: int | None = ..., keepdim: bool = ...) -> "Tensor": ...
    def sum(
        self,
        dim: int | None = ...,
        keepdim: bool = ...,
        *,
        dtype: _DType | None = ...,
    ) -> "Tensor": ...
    def unsqueeze(self, dim: int) -> "Tensor": ...
    def squeeze(self, dim: int | None = ...) -> "Tensor": ...
    def transpose(self, dim0: int, dim1: int) -> "Tensor": ...

    # Arithmetic -------------------------------------------------------------
    def __add__(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    __radd__ = __add__
    def __sub__(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    __rsub__ = __sub__
    def __mul__(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    __rmul__ = __mul__
    def __truediv__(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    def __rtruediv__(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    def __matmul__(self, other: "Tensor") -> "Tensor": ...

    # In‑place ops -----------------------------------------------------------
    def add_(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    def mul_(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    def sub_(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    def copy_(
        self: _TTensor,
        other: "Tensor",
        *,
        non_blocking: bool | None = ...,
    ) -> _TTensor: ...
    def zero_(self) -> "Tensor": ...
    def fill_(self, value: _b.int | _b.float) -> "Tensor": ...

    # Comparisons / boolean logic -------------------------------------------
    def __lt__(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    def __le__(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    def __gt__(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    def __ge__(self, other: "Tensor | _b.float | _b.int") -> "Tensor": ...
    def __and__(self, other: "Tensor | bool") -> "Tensor": ...
    __rand__ = __and__

    # Slicing / indexing
    def __getitem__(self, idx: object) -> "Tensor": ...

    # Utilities --------------------------------------------------------------
    def detach(self) -> "Tensor": ...
    def cpu(self) -> "Tensor": ...
    def clone(self) -> "Tensor": ...
    def reshape(
        self,
        shape: Tuple[int, ...] | Sequence[int] | int,
        *more: int,
    ) -> "Tensor": ...
    def tolist(self) -> list[_b.int | _b.float]: ...
    def item(self) -> _b.int | _b.float: ...
    def to(
        self: _TTensor,
        dtype: _DType | None = ...,
        device: _TorchDevice | str | None = ...,
        copy: bool | None = ...,
        non_blocking: bool | None = ...,
    ) -> _TTensor: ...
    def equal(self, other: "Tensor") -> bool: ...
    def __iter__(self) -> Iterator["Tensor"]: ...

    # Storage & memory helpers (added for tests) --------------------------
    def untyped_storage(self) -> _UntypedStorage: ...
    def is_pinned(self) -> bool: ...

# ───────────────────────── functional helpers (top level) ──────────────────
#  Re‑export the Generator so user code can do torch.Generator().
from .random import Generator as Generator  # noqa: E402

def is_floating_point(t: Tensor, /) -> bool: ...
def empty(
    *size: int,
    dtype: _DType | None = ...,
    device: _TorchDevice | str | None = ...,
    pin_memory: bool | None = ...,
    requires_grad: bool = ...,
) -> Tensor: ...
def zeros(*size: int, dtype: _DType | None = ...) -> Tensor: ...
def ones(
    *size: int, dtype: _DType | None = ..., device: _TorchDevice | str | None = ...
) -> Tensor: ...
def full(
    size: Tuple[int, ...],
    fill_value: _b.int | _b.float,
    dtype: _DType | None = ...,
) -> Tensor: ...
def full_like(
    a: Tensor,
    fill_value: _b.int | _b.float,
    *,
    dtype: _DType | None = ...,
    device: _TorchDevice | str | None = ...,
) -> Tensor: ...
def zeros_like(a: Tensor) -> Tensor: ...
def randn(
    *size: int,
    dtype: _DType | None = ...,
    device: _TorchDevice | str | None = ...,
    requires_grad: bool = ...,
    generator: "Generator | None" = ...,
) -> Tensor: ...
def arange(
    start: _b.int | _b.float,
    end: _b.int | _b.float | None = ...,
    step: _b.int | _b.float = ...,
    *,
    dtype: _DType | None = ...,
    device: _TorchDevice | str | None = ...,
    requires_grad: bool = ...,
    generator: "Generator | None" = ...,
) -> Tensor: ...
def tensor(
    data: object,
    *,
    dtype: _DType | None = ...,
    device: _TorchDevice | str | None = ...,
    requires_grad: bool = ...,
) -> Tensor: ...
def matmul(a: Tensor, b: Tensor) -> Tensor: ...
def sqrt(a: Tensor) -> Tensor: ...
def clamp(
    a: Tensor, *, min: _b.float | None = ..., max: _b.float | None = ...
) -> Tensor: ...
def relu(a: Tensor) -> Tensor: ...
def stack(tensors: Sequence[Tensor], dim: int = ...) -> Tensor: ...
def square(a: Tensor) -> Tensor: ...
def all(a: Tensor) -> bool: ...
def allclose(
    a: Tensor, b: Tensor, atol: _b.float = ..., rtol: _b.float = ...
) -> bool: ...
def isfinite(a: Tensor) -> Tensor: ...
def manual_seed(seed: int) -> None: ...
def equal(a: Tensor, b: Tensor, /) -> bool: ...

# complex‑number helpers
def real(a: Tensor) -> Tensor: ...
def imag(a: Tensor) -> Tensor: ...
def view_as_complex(a: Tensor) -> Tensor: ...

abs = Tensor.abs  # make ``torch.abs`` available

# ───────────────────────────── autograd guard ─────────────────────────────
class _NoGrad:
    def __enter__(self) -> None: ...
    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc_val: BaseException | None,
        exc_tb: object | None,
    ) -> None: ...

def no_grad() -> _NoGrad: ...

# ─────────────────────────── sub‑package re‑exports ───────────────────────
from . import nn as nn  # noqa: F401 – used at runtime
from . import optim as optim  # noqa: F401
from . import cuda as cuda  # noqa: F401
from . import utils as utils  # noqa: F401
from . import fft as fft  # runtime sub‑module
from . import linalg as linalg
from . import random as random  # noqa: F401

# ────────────────────────── deterministic helper ─────────────────────────
def use_deterministic_algorithms(
    mode: bool, *, warn_only: bool | None = ...
) -> None: ...

# ───────────────────────────── version info ───────────────────────────────
__version__: str
from types import ModuleType as _ModuleType  # noqa: E402
import importlib as _il  # noqa: E402

version: _ModuleType = _il.import_module("torch.version")
