from __future__ import annotations

from types import ModuleType
from typing import Sequence, Tuple, overload
import builtins as _b  # keep built-in float / int distinct

# ────── value-level dtype sentinels ──────
class dtype: ...

float32: dtype
float64: dtype
int64: dtype
float16: dtype
bfloat16: dtype
complex64: dtype
complex128: dtype

# runtime aliases that real torch exports
float: dtype  # torch.float32
double: dtype  # torch.float64
long: dtype  # torch.int64

# ────── minimal torch.device sentinel ──────
class device:
    def __init__(self, type: str, index: int | None = ...): ...

# ────── minimal Tensor object (only methods/operators used) ──────
class Tensor:
    # construction
    def __init__(
        self,
        *size: int,
        dtype: dtype | None = ...,
        requires_grad: bool = ...,
    ) -> None: ...

    # shape helpers
    def mean(self, dim: int | None = ..., **kw: object) -> Tensor: ...
    def unsqueeze(self, dim: int) -> Tensor: ...
    def squeeze(self, dim: int | None = ...) -> Tensor: ...
    def transpose(self, dim0: int, dim1: int) -> Tensor: ...
    @property
    def T(self) -> Tensor: ...

    # in-place ops
    def mul_(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def add_(self, other: Tensor | _b.float | _b.int) -> Tensor: ...

    # arithmetic
    def __add__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    __radd__ = __add__
    def __sub__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    __rsub__ = __sub__
    def __mul__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    __rmul__ = __mul__
    def __truediv__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __rtruediv__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __matmul__(self, other: Tensor) -> Tensor: ...

    # comparisons → boolean masks
    def __lt__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __le__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __gt__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...
    def __ge__(self, other: Tensor | _b.float | _b.int) -> Tensor: ...

    # logical masks
    def __and__(self, other: Tensor | bool) -> Tensor: ...
    __rand__ = __and__

    # indexing
    def __getitem__(self, item: object) -> Tensor: ...

    # ── attributes / helpers used by the serializer ──
    @property
    def dtype(self) -> dtype: ...
    @property
    def shape(self) -> tuple[int, ...]: ...
    @property
    def ndim(self) -> int: ...
    def detach(self) -> Tensor: ...
    def cpu(self) -> Tensor: ...
    def clone(self) -> Tensor: ...
    def reshape(
        self, shape: tuple[int, ...] | list[int] | int, *more: int
    ) -> Tensor: ...
    def tolist(self) -> list[_b.int | _b.float]: ...
    def item(self) -> _b.int | _b.float: ...

# ────── functional helpers referenced in cvnn.py ──────
def zeros(*size: int, dtype: dtype | None = ...) -> Tensor: ...
def full(
    size: Tuple[int, ...],
    fill_value: _b.int | _b.float,
    dtype: dtype | None = ...,
) -> Tensor: ...
def matmul(a: Tensor, b: Tensor) -> Tensor: ...
def sqrt(a: Tensor) -> Tensor: ...
def clamp(
    a: Tensor,
    *,
    min: _b.int | _b.float | None = ...,
    max: _b.int | _b.float | None = ...,
) -> Tensor: ...
def relu(a: Tensor) -> Tensor: ...
def stack(tensors: Sequence[Tensor], dim: int = ...) -> Tensor: ...

# ────── factory helpers ──────
@overload
def tensor(
    data: _b.int | _b.float,
    *,
    dtype: dtype | None = ...,
    device: device | str | None = ...,
) -> Tensor: ...
@overload
def tensor(
    data: Sequence[_b.int | _b.float],
    *,
    dtype: dtype | None = ...,
    device: device | str | None = ...,
) -> Tensor: ...

# ────── no_grad context manager ──────
class _NoGrad:
    def __enter__(self) -> None: ...
    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc: BaseException | None,
        tb: object | None,
    ) -> None: ...

def no_grad() -> _NoGrad: ...

# ────── placeholder sub-modules so that
#        import torch.nn as nn
#        torch.linalg.eigh
#        both resolve (their detailed stubs live below) ──────
nn: ModuleType
linalg: ModuleType
