# File: documents/whitepapers/deep_complex_valued_set_encoders.md
# Deep Complex-Valued Set Encoders for Dupire Local Volatility Option Pricing

**Status**: Reference only  
**Supersedes**: Prior set encoder explorations  
**Referenced by**: documents/domain/index.md

> **Purpose**: Survey complex-valued set encoder approaches for variable-length local volatility surfaces.
> **ðŸ“– Authoritative Reference**: [../documentation_standards.md](../documentation_standards.md)

---

## Table of Contents
1. [Introduction](#introduction)  
2. [Feature Preparation with Random Fourier Features](#feature-preparation-with-random-fourier-features)  
3. [Architectures](#architectures)  
4. [Endâ€‘toâ€‘EndÂ PyTorch Implementation Walkâ€‘through](#end-to-end-pytorch-implementation-walk-through)  
5. [ExtensionsÂ &Â PracticalÂ Tips](#extensions--practical-tips)  
6. [Conclusion](#conclusion)  

---

<a name="introduction"></a>
## 1Â Â Introduction

### 1.1Â Â PointÂ Clouds as Localâ€‘Vol Surfaces  
A **point cloud** is an unordered set of points

$$\{x_i\}_{i=1}^N \subset \mathbb{R}^d$$

Most deepâ€‘learning layers assume a *fixed ordering*, but point clouds are **permutation invariant**:

$$f\big(\{x_{\pi(i)}\}\big) = f\big(\{x_i\}\big) \quad \forall\,\text{permutations } \pi$$

In option pricing under a **Dupire localâ€‘volatility model**, each market surface is an irregular set of tuples  

$$x_i = (K_i,\; T_i,\; \sigma^{\text{loc}}_i)$$

where $K$ is strike, $T$ maturity, and $\sigma^{\text{loc}}$ the local volatility.  The number and placement of quotes varies dayâ€‘toâ€‘day, so we need a model that handles **variableâ€‘length, unordered inputs**.

### 1.2Â Â Transformers for Sets  
Transformers replace recurrence with **selfâ€‘attention**, allowing every token to attend to every other.  If we drop positional encodings (or use contentâ€‘based encodings), attention becomes **orderÂ agnostic** (SetÂ Transformer,Â 2019).

### 1.3Â Â Complexâ€‘ValuedÂ NetworksÂ &Â *modReLU*  
Our network outputs the complex **characteristic function**

$$\varphi(u) = \mathbb{E}\big[e^{i u X_T}\big]$$

so we keep tensors **complex throughout** and use the *modReLU* activation:

```python
# File: examples/cvnn/modrelu.py
def modrelu(z, b):
    r = torch.abs(z)
    return torch.relu(r + b) * torch.exp(1j * torch.angle(z))
```

---

<a name="feature-preparation-with-random-fourier-features"></a>
## 2Â Â FeatureÂ Preparation with RandomÂ FourierÂ Features

### 2.1Â Â Motivation  
Raw coordinates make it hard for a network to learn highâ€‘frequency structure.  **Random Fourier Features (RFF)** embed inputs in a richer basis:

$$\phi_{\boldsymbol\omega}(x) = e^{i\,\boldsymbol\omega^{\top} x}, \quad \boldsymbol\omega \sim \mathcal{N}(0,\Sigma)$$

### 2.2Â Â Complex RFF for $(K,T)$  
Work in **logâ€‘moneyness** $m = \log(K/S_0)$ and maturity $T$.  
Draw $D$ frequencies $\{\omega_j\}$ from a suitable spread.  
Define  

$$\text{RFF}(m,T) = \big[e^{i\omega_1 m},\; e^{i\omega_1 T},\;\dots, e^{i\omega_D T}\big]$$

Concatenate the scalar local vol:

$$x_i = \sigma^{\text{loc}}_i \;\otimes\; \text{RFF}(m_i,T_i) \;\in\; \mathbb{C}^{2D}$$

### 2.3Â Â MinimalÂ PyTorch Snippet
```python
# File: examples/cvnn/complex_rff.py
class ComplexRFF(nn.Module):
    def __init__(self, d=12, base=1.0):
        super().__init__()
        freqs = base * torch.logspace(0, 2, d)
        self.register_buffer("freqs", freqs)

    def forward(self, m, T):
        phase_m = m.unsqueeze(-1) * self.freqs
        phase_t = T.unsqueeze(-1) * self.freqs
        return torch.cat([torch.exp(1j*phase_m),
                          torch.exp(1j*phase_t)], dim=-1)
```

---

<a name="architectures"></a>
## 3Â Â Architectures

### 3.1Â Â ComplexÂ PointNetÂ /Â DeepSets
1. **SharedÂ $\phi$** â€“ complex MLP with *modReLU*.  
2. **Pooling** â€“ mean/sum across points â‡’ permutation invariance.  
3. **GlobalÂ $\rho$** â€“ complex MLP mapping pooled vector to latent code $h$.

### 3.2Â Â ComplexÂ SetÂ Transformer  
Selfâ€‘attention weights  

$$\alpha_{ij} = \text{softmax}\big(\operatorname{Re}(Q_i K_j^{\dagger})\big)$$

give explicit pair interactions; Poolingâ€‘byâ€‘Multiâ€‘Head Attention extracts global code.

### 3.3Â Â Tradeâ€‘offs  

|               | PointNet | SetÂ Transformer |
|---------------|----------|-----------------|
| Complexity    | $\mathcal{O}(N)$ | $\mathcal{O}(N^2)$ |
| Interactions  | implicit | explicit |
| Simplicity    | easier   | heavier |

---

<a name="end-to-end-pytorch-implementation-walk-through"></a>
## 4Â Â Endâ€‘toâ€‘EndÂ PyTorch Walkâ€‘through

```python
# File: examples/cvnn/complex_pointnet_stub.py
# ComplexLinear, ModReLU, ComplexPointNet, CharFuncHead defined as in text â€¦
```

```python
# File: examples/cvnn/complex_pointnet_training.py
def fake_batch(B=16, N=40, D=12):
    m   = torch.randn(B, N)
    T   = 2.0 * torch.rand(B, N)
    sig = 0.1 + 0.4 * torch.rand(B, N)
    rff = ComplexRFF(D)
    pts = sig.unsqueeze(-1) * rff(m, T)          # (B,N,2D)
    target = torch.randn(B, 32, dtype=torch.cfloat)
    return pts, target

model = nn.Sequential(
    ComplexPointNet(point_dim=24),
    CharFuncHead()
)
opt = torch.optim.Adam(model.parameters(), 1e-3)
for step in range(200):
    x, t = fake_batch()
    y = model(x)
    loss = torch.mean(torch.abs(y - t) ** 2)
    opt.zero_grad(); loss.backward(); opt.step()
```

---

<a name="extensions--practical-tips"></a>
## 5Â Â ExtensionsÂ &Â Tips
* Martingale penalty: add $|\varphi(0)-1|^2$.  
* Frequency curriculum: start lowâ€‘freq, add highs progressively.  
* Complex LayerNorm: normalize magnitudes only.  
* Crossâ€‘attention: query the vol set with target $(K^*,T^*)$.

---

<a name="conclusion"></a>
## 6Â Â Conclusion
Fully complex, permutationâ€‘invariant encoders built on **PointNet** or **SetÂ Transformer**, combined with **Random Fourier Features** and *modReLU*, offer an elegant path to neural Dupire pricers that output characteristic functions.
